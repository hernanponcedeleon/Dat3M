BPF C-RW-r+RW-a+RW-B+RW-B+RW-B+RW-B+RW-B+RW-B-BPF
(*
 *  * Result: Never
 *  * 
 *  * Process 0 starts (t=100000).
 *  * 
 *  * P0 advances slightly (t=100001).
 *  * 
 *  * P1 advances slightly (t=100003).
 *  * 
 *  * P2 advances slightly (t=100005).
 *  * 
 *  * P3 advances slightly (t=100007).
 *  * 
 *  * P4 advances slightly (t=100009).
 *  * 
 *  * P5 advances slightly (t=100011).
 *  * 
 *  * P6 advances slightly (t=100013).
 *  * 
 *  * P7 advances slightly (t=100015).
 *  * 
 *  * Process 0 start at t=100000, process 8 end at t=100015: Cycle forbidden.
 *  *
 *  * 0:r1 -> 0:r1
 *  * 0:-tmp- -> 0:r2
 *  * 1:r1 -> 1:r1
 *  * 2:r1 -> 2:r1
 *  * 2:-tmp- -> 2:r2
 *  * 3:r1 -> 3:r1
 *  * 3:-tmp- -> 3:r2
 *  * 4:r1 -> 4:r1
 *  * 4:-tmp- -> 4:r2
 *  * 5:r1 -> 5:r1
 *  * 5:-tmp- -> 5:r2
 *  * 6:r1 -> 6:r1
 *  * 6:-tmp- -> 6:r2
 *  * 7:r1 -> 7:r1
 *  * 7:-tmp- -> 7:r2
 *)
{
	0:r10 = x0;
	0:r9 = x1;
	1:r10 = x1;
	1:r9 = x2;
	2:r10 = x2;
	2:r9 = x3;
	2:r8 = __temporary_2;
	3:r10 = x3;
	3:r9 = x4;
	3:r8 = __temporary_3;
	4:r10 = x4;
	4:r9 = x5;
	4:r8 = __temporary_4;
	5:r10 = x5;
	5:r9 = x6;
	5:r8 = __temporary_5;
	6:r10 = x6;
	6:r9 = x7;
	6:r8 = __temporary_6;
	7:r10 = x7;
	7:r9 = x0;
	7:r8 = __temporary_7;
}
 P0                                 | P1                                  | P2                                        | P3                                        | P4                                        | P5                                        | P6                                        | P7                                        ;
 r1 = *(u32 *)(r10 + 0)             | r1 = load_acquire((u32 *)(r10 + 0)) | r1 = *(u32 *)(r10 + 0)                    | r1 = *(u32 *)(r10 + 0)                    | r1 = *(u32 *)(r10 + 0)                    | r1 = *(u32 *)(r10 + 0)                    | r1 = *(u32 *)(r10 + 0)                    | r1 = *(u32 *)(r10 + 0)                    ;
 r2 = 1                             | *(u32 *)(r9 + 0) = 1                | r2 = atomic_fetch_add((u64*)(r8 + 0), r2) | r2 = atomic_fetch_add((u64*)(r8 + 0), r2) | r2 = atomic_fetch_add((u64*)(r8 + 0), r2) | r2 = atomic_fetch_add((u64*)(r8 + 0), r2) | r2 = atomic_fetch_add((u64*)(r8 + 0), r2) | r2 = atomic_fetch_add((u64*)(r8 + 0), r2) ;
 store_release((u32 *)(r9 + 0), r2) |                                     | *(u32 *)(r9 + 0) = 1                      | *(u32 *)(r9 + 0) = 1                      | *(u32 *)(r9 + 0) = 1                      | *(u32 *)(r9 + 0) = 1                      | *(u32 *)(r9 + 0) = 1                      | *(u32 *)(r9 + 0) = 1                      ;
exists
((0:r1=1 /\ 1:r1=1 /\ 2:r1=1 /\ 3:r1=1 /\ 4:r1=1 /\ 5:r1=1 /\ 6:r1=1 /\ 7:r1=1))

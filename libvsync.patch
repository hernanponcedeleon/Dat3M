diff --git a/include/vsync/spinlock/cnalock.h b/include/vsync/spinlock/cnalock.h
index 75b3e3a..7f749ba 100644
--- a/include/vsync/spinlock/cnalock.h
+++ b/include/vsync/spinlock/cnalock.h
@@ -185,7 +185,7 @@ cnalock_release(cnalock_t *lock, cna_node_t *me, vuint32_t numa_node)
             cna_node_t *sec_head = vatomicptr_xchg_rlx(&sec_tail->next, NULL);
 
             // Access to tail
-            if (vatomicptr_cmpxchg_rel(&lock->tail, me, sec_tail) == me) {
+            if (vatomicptr_cmpxchg_rlx(&lock->tail, me, sec_tail) == me) {
                 // Access to another thread queue element -- spin
                 vatomicptr_write_rel(&sec_head->spin, (void *)1);
                 return;
diff --git a/include/vsync/spinlock/hclhlock.h b/include/vsync/spinlock/hclhlock.h
index 1df3536..3fca130 100644
--- a/include/vsync/spinlock/hclhlock.h
+++ b/include/vsync/spinlock/hclhlock.h
@@ -181,7 +181,7 @@ hclhlock_acquire(hclh_lock_t *lock, hclh_tnode_t *tnode)
     // splice my qnode into my cluster's local queue
     do {
         exp_tail = pred;
-        pred     = vatomicptr_cmpxchg(local_queue, exp_tail, tnode->qnode);
+        pred     = vatomicptr_cmpxchg_rlx(local_queue, exp_tail, tnode->qnode);
     } while (pred != exp_tail);
 
     // if I have a predecessor I will wait till its done or I am the cluster
diff --git a/include/vsync/spinlock/hmcslock.h b/include/vsync/spinlock/hmcslock.h
index e22b0eb..c57d64e 100644
--- a/include/vsync/spinlock/hmcslock.h
+++ b/include/vsync/spinlock/hmcslock.h
@@ -296,7 +296,7 @@ _hmcslock_acquire_real(hmcslock_t *lock, hmcs_node_t *qnode, vsize_t depth)
     // Prepare the node for use
     vatomic64_write_rlx(&qnode->status, HMCLOCK_WAIT);
     vatomicptr_write_rlx(&qnode->next, NULL);
-    pred = (hmcs_node_t *)vatomicptr_xchg(&lock->lock, qnode);
+    pred = (hmcs_node_t *)vatomicptr_xchg_rlx(&lock->lock, qnode);
     if (pred) {
         vatomicptr_write_rel(&pred->next, qnode);
         // spin as long as the status is wait
diff --git a/include/vsync/spinlock/mcslock.h b/include/vsync/spinlock/mcslock.h
index 571af8f..75d3a36 100644
--- a/include/vsync/spinlock/mcslock.h
+++ b/include/vsync/spinlock/mcslock.h
@@ -74,7 +74,7 @@ mcslock_tryacquire(mcslock_t *l, mcs_node_t *node)
     vatomicptr_write_rlx(&node->next, NULL);
     vatomic32_write_rlx(&node->locked, 1);
 
-    pred = (mcs_node_t *)vatomicptr_cmpxchg(&l->tail, NULL, node);
+    pred = (mcs_node_t *)vatomicptr_cmpxchg_acq(&l->tail, NULL, node);
 
     return pred == NULL;
 }
@@ -93,7 +93,7 @@ mcslock_acquire(mcslock_t *l, mcs_node_t *node)
     vatomicptr_write_rlx(&node->next, NULL);
     vatomic32_write_rlx(&node->locked, 1);
 
-    pred = (mcs_node_t *)vatomicptr_xchg(&l->tail, node);
+    pred = (mcs_node_t *)vatomicptr_xchg_acq(&l->tail, node);
     if (pred) {
         vatomicptr_write_rel(&pred->next, node);
         vatomic32_await_eq_acq(&node->locked, 0);
diff --git a/include/vsync/spinlock/twalock.h b/include/vsync/spinlock/twalock.h
index 087c470..e645308 100644
--- a/include/vsync/spinlock/twalock.h
+++ b/include/vsync/spinlock/twalock.h
@@ -129,7 +129,7 @@ static inline void
 _twalock_acquire_slowpath(twalock_t *l, vuint32_t t)
 {
     vuint32_t at = TWA_HASH(t);
-    vuint32_t u  = vatomic32_read_acq(TWA_ARRAY(at));
+    vuint32_t u  = vatomic32_read_rlx(TWA_ARRAY(at));
     vuint32_t k  = vatomic32_read_rlx(&l->grant);
     vuint32_t dx = TWA_DIFF(t, k);
 
